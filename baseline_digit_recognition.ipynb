{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Deep Learning\n",
    "## Project: Build a Digit Recognition Program\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 1: Design and Test a Model Architecture\n",
    "Design and implement a deep learning model that learns to recognize sequences of digits. Train the model using synthetic data generated by concatenating character images from [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) or [MNIST](http://yann.lecun.com/exdb/mnist/). To produce a synthetic sequence of digits for testing, you can for example limit yourself to sequences up to five digits, and use five classifiers on top of your deep network. You would have to incorporate an additional ‘blank’ character to account for shorter number sequences.\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "- Your model can be derived from a deep neural net or a convolutional network.\n",
    "- You could experiment sharing or not the weights between the softmax classifiers.\n",
    "- You can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf). ([video](https://www.youtube.com/watch?v=vGPI_JvLoN0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test imports\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training:', (200000, 28, 28), (200000,))\n",
      "('Validation:', (10000, 28, 28), (10000,))\n",
      "('Testing:', (10000, 28, 28), (10000,))\n"
     ]
    }
   ],
   "source": [
    "from notMNIST_download import MNIST_setup\n",
    "train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels = MNIST_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 28, 28) (200000,) (10000, 28, 28) (10000,) (10000, 28, 28) (10000,)\n",
      "(200000, 28, 28, 1) (200000, 11) (10000, 28, 28, 1) (10000, 11) (10000, 28, 28, 1) (10000, 11)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape, train_labels.shape, \n",
    "      test_dataset.shape, test_labels.shape, \n",
    "      valid_dataset.shape, valid_labels.shape)\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  image_size = 28\n",
    "  num_channels = 1\n",
    "  num_labels = 11\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print(train_dataset.shape, train_labels.shape, \n",
    "      test_dataset.shape, test_labels.shape, \n",
    "      valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def dataBatch(data, labels, batchSize = 16):\n",
    "    limit = len(labels)\n",
    "    rChoice = []\n",
    "    for i in range(batchSize):\n",
    "        rChoice.append(random.randrange(0, limit, 1))\n",
    "    batchX = np.take(data, rChoice, axis=0)\n",
    "    batchX = batchX.reshape(batchX.shape[0], batchX.shape[1]**2)\n",
    "    batchY = np.take(labels, rChoice, axis=0)\n",
    "    return batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784], name=\"data\")\n",
    "y_ = tf.placeholder(tf.float32, [None, 11], name=\"true_labels\")\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 11])\n",
    "b_fc2 = bias_variable([11])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08\n",
      "step 100, training accuracy 0.74\n",
      "step 200, training accuracy 0.7\n",
      "step 300, training accuracy 0.74\n",
      "step 400, training accuracy 0.84\n",
      "step 500, training accuracy 0.8\n",
      "step 600, training accuracy 0.92\n",
      "step 700, training accuracy 0.86\n",
      "step 800, training accuracy 0.88\n",
      "step 900, training accuracy 0.78\n",
      "step 1000, training accuracy 0.76\n",
      "step 1100, training accuracy 0.84\n",
      "step 1200, training accuracy 0.84\n",
      "step 1300, training accuracy 0.92\n",
      "step 1400, training accuracy 0.82\n",
      "step 1500, training accuracy 0.84\n",
      "step 1600, training accuracy 0.86\n",
      "step 1700, training accuracy 0.88\n",
      "step 1800, training accuracy 0.84\n",
      "step 1900, training accuracy 0.92\n",
      "step 2000, training accuracy 0.88\n",
      "step 2100, training accuracy 0.86\n",
      "step 2200, training accuracy 0.88\n",
      "step 2300, training accuracy 0.82\n",
      "step 2400, training accuracy 0.84\n",
      "step 2500, training accuracy 0.9\n",
      "step 2600, training accuracy 0.9\n",
      "step 2700, training accuracy 0.84\n",
      "step 2800, training accuracy 0.82\n",
      "step 2900, training accuracy 0.82\n",
      "step 3000, training accuracy 0.94\n",
      "step 3100, training accuracy 0.92\n",
      "step 3200, training accuracy 0.92\n",
      "step 3300, training accuracy 0.98\n",
      "step 3400, training accuracy 0.86\n",
      "step 3500, training accuracy 0.92\n",
      "step 3600, training accuracy 0.9\n",
      "step 3700, training accuracy 0.86\n",
      "step 3800, training accuracy 0.86\n",
      "step 3900, training accuracy 0.94\n",
      "step 4000, training accuracy 0.88\n",
      "step 4100, training accuracy 0.88\n",
      "step 4200, training accuracy 0.8\n",
      "step 4300, training accuracy 0.96\n",
      "step 4400, training accuracy 0.9\n",
      "step 4500, training accuracy 0.96\n",
      "step 4600, training accuracy 0.9\n",
      "step 4700, training accuracy 0.92\n",
      "step 4800, training accuracy 0.92\n",
      "step 4900, training accuracy 0.84\n",
      "step 5000, training accuracy 0.9\n",
      "step 5100, training accuracy 0.94\n",
      "step 5200, training accuracy 0.9\n",
      "step 5300, training accuracy 0.88\n",
      "step 5400, training accuracy 0.92\n",
      "step 5500, training accuracy 0.86\n",
      "step 5600, training accuracy 0.92\n",
      "step 5700, training accuracy 0.96\n",
      "step 5800, training accuracy 0.88\n",
      "step 5900, training accuracy 0.9\n",
      "step 6000, training accuracy 0.88\n",
      "step 6100, training accuracy 0.9\n",
      "step 6200, training accuracy 0.8\n",
      "step 6300, training accuracy 0.92\n",
      "step 6400, training accuracy 0.98\n",
      "step 6500, training accuracy 0.92\n",
      "step 6600, training accuracy 0.9\n",
      "step 6700, training accuracy 0.96\n",
      "step 6800, training accuracy 0.96\n",
      "step 6900, training accuracy 0.9\n",
      "step 7000, training accuracy 0.9\n",
      "step 7100, training accuracy 0.9\n",
      "step 7200, training accuracy 0.9\n",
      "step 7300, training accuracy 0.88\n",
      "step 7400, training accuracy 0.94\n",
      "step 7500, training accuracy 0.94\n",
      "step 7600, training accuracy 0.92\n",
      "step 7700, training accuracy 0.9\n",
      "step 7800, training accuracy 0.84\n",
      "step 7900, training accuracy 0.86\n",
      "step 8000, training accuracy 0.94\n",
      "step 8100, training accuracy 0.98\n",
      "step 8200, training accuracy 0.96\n",
      "step 8300, training accuracy 0.98\n",
      "step 8400, training accuracy 0.88\n",
      "step 8500, training accuracy 0.92\n",
      "step 8600, training accuracy 0.88\n",
      "step 8700, training accuracy 0.82\n",
      "step 8800, training accuracy 0.88\n",
      "step 8900, training accuracy 0.9\n",
      "step 9000, training accuracy 0.96\n",
      "step 9100, training accuracy 0.9\n",
      "step 9200, training accuracy 0.96\n",
      "step 9300, training accuracy 0.96\n",
      "step 9400, training accuracy 0.94\n",
      "step 9500, training accuracy 0.94\n",
      "step 9600, training accuracy 0.94\n",
      "step 9700, training accuracy 0.98\n",
      "step 9800, training accuracy 0.92\n",
      "step 9900, training accuracy 0.98\n",
      "step 10000, training accuracy 0.92\n",
      "step 10100, training accuracy 0.92\n",
      "step 10200, training accuracy 0.96\n",
      "step 10300, training accuracy 0.96\n",
      "step 10400, training accuracy 0.92\n",
      "step 10500, training accuracy 0.86\n",
      "step 10600, training accuracy 0.86\n",
      "step 10700, training accuracy 0.94\n",
      "step 10800, training accuracy 0.94\n",
      "step 10900, training accuracy 0.9\n",
      "step 11000, training accuracy 0.9\n",
      "step 11100, training accuracy 0.94\n",
      "step 11200, training accuracy 0.94\n",
      "step 11300, training accuracy 0.96\n",
      "step 11400, training accuracy 1\n",
      "step 11500, training accuracy 0.9\n",
      "step 11600, training accuracy 0.94\n",
      "step 11700, training accuracy 0.86\n",
      "step 11800, training accuracy 0.94\n",
      "step 11900, training accuracy 0.96\n",
      "step 12000, training accuracy 0.98\n",
      "step 12100, training accuracy 0.92\n",
      "step 12200, training accuracy 0.9\n",
      "step 12300, training accuracy 0.94\n",
      "step 12400, training accuracy 0.94\n",
      "step 12500, training accuracy 0.88\n",
      "step 12600, training accuracy 0.94\n",
      "step 12700, training accuracy 0.82\n",
      "step 12800, training accuracy 1\n",
      "step 12900, training accuracy 0.98\n",
      "step 13000, training accuracy 0.96\n",
      "step 13100, training accuracy 0.86\n",
      "step 13200, training accuracy 0.92\n",
      "step 13300, training accuracy 0.96\n",
      "step 13400, training accuracy 0.92\n",
      "step 13500, training accuracy 0.88\n",
      "step 13600, training accuracy 0.94\n",
      "step 13700, training accuracy 0.92\n",
      "step 13800, training accuracy 0.98\n",
      "step 13900, training accuracy 0.98\n",
      "step 14000, training accuracy 0.88\n",
      "step 14100, training accuracy 0.98\n",
      "step 14200, training accuracy 0.96\n",
      "step 14300, training accuracy 0.94\n",
      "step 14400, training accuracy 0.9\n",
      "step 14500, training accuracy 0.96\n",
      "step 14600, training accuracy 0.88\n",
      "step 14700, training accuracy 0.98\n",
      "step 14800, training accuracy 0.94\n",
      "step 14900, training accuracy 0.92\n",
      "step 15000, training accuracy 0.9\n",
      "step 15100, training accuracy 0.88\n",
      "step 15200, training accuracy 0.92\n",
      "step 15300, training accuracy 0.94\n",
      "step 15400, training accuracy 0.96\n",
      "step 15500, training accuracy 0.92\n",
      "step 15600, training accuracy 0.94\n",
      "step 15700, training accuracy 0.92\n",
      "step 15800, training accuracy 0.92\n",
      "step 15900, training accuracy 0.92\n",
      "step 16000, training accuracy 0.92\n",
      "step 16100, training accuracy 0.98\n",
      "step 16200, training accuracy 0.92\n",
      "step 16300, training accuracy 1\n",
      "step 16400, training accuracy 0.92\n",
      "step 16500, training accuracy 0.88\n",
      "step 16600, training accuracy 0.9\n",
      "step 16700, training accuracy 0.96\n",
      "step 16800, training accuracy 0.94\n",
      "step 16900, training accuracy 0.88\n",
      "step 17000, training accuracy 0.92\n",
      "step 17100, training accuracy 0.96\n",
      "step 17200, training accuracy 0.94\n",
      "step 17300, training accuracy 0.92\n",
      "step 17400, training accuracy 0.96\n",
      "step 17500, training accuracy 0.98\n",
      "step 17600, training accuracy 0.92\n",
      "step 17700, training accuracy 0.9\n",
      "step 17800, training accuracy 0.9\n",
      "step 17900, training accuracy 0.88\n",
      "step 18000, training accuracy 0.94\n",
      "step 18100, training accuracy 0.94\n",
      "step 18200, training accuracy 0.96\n",
      "step 18300, training accuracy 0.9\n",
      "step 18400, training accuracy 0.92\n",
      "step 18500, training accuracy 0.96\n",
      "step 18600, training accuracy 0.98\n",
      "step 18700, training accuracy 0.92\n",
      "step 18800, training accuracy 0.96\n",
      "step 18900, training accuracy 0.96\n",
      "step 19000, training accuracy 0.9\n",
      "step 19100, training accuracy 0.94\n",
      "step 19200, training accuracy 0.9\n",
      "step 19300, training accuracy 0.92\n",
      "step 19400, training accuracy 0.98\n",
      "step 19500, training accuracy 0.88\n",
      "step 19600, training accuracy 0.94\n",
      "step 19700, training accuracy 0.92\n",
      "step 19800, training accuracy 1\n",
      "step 19900, training accuracy 0.96\n",
      "test accuracy 0.9648\n"
     ]
    }
   ],
   "source": [
    "# Classifier Function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "with sess.as_default():\n",
    "    for i in range(20000):\n",
    "      batch = dataBatch(train_dataset, train_labels, 50)\n",
    "      if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "      train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    print(\"test accuracy %g\"%accuracy.eval(\n",
    "            feed_dict={x: test_dataset.reshape(\n",
    "                    test_dataset.shape[0], test_dataset.shape[1]**2), \n",
    "                       y_: test_labels, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "_How did you train your model? How did you generate your synthetic dataset?_ Include examples of images from the synthetic data you constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 2: Train a Model on a Realistic Dataset\n",
    "Once you have settled on a good architecture, you can train your model on real data. In particular, the [Street View House Numbers (SVHN)](http://ufldl.stanford.edu/housenumbers/) dataset is a good large-scale dataset collected from house numbers in Google Street View. Training on this more challenging dataset, where the digits are not neatly lined-up and have various skews, fonts and colors, likely means you have to do some hyperparameter exploration to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "_Describe how you set up the training and testing data for your model. How does the model perform on a realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "_What changes did you have to make, if any, to achieve \"good\" results? Were there any options you explored that made the results worse?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "_What were your initial and final results with testing on a realistic dataset? Do you believe your model is doing a good enough job at classifying numbers correctly?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 3: Test a Model on Newly-Captured Images\n",
    "\n",
    "Take several pictures of numbers that you find around you (at least five), and run them through your classifier on your computer to produce example results. Alternatively (optionally), you can try using OpenCV / SimpleCV / Pygame to capture live images from a webcam and run those through your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "_Choose five candidate images of numbers you took from around you and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Leave blank if you did not complete this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Step 4: Explore an Improvement for a Model\n",
    "\n",
    "There are many things you can do once you have the basic classifier in place. One example would be to also localize where the numbers are on the image. The SVHN dataset provides bounding boxes that you can tune to train a localizer. Train a regression loss to the coordinates of the bounding box, and then test it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "_How well does your model localize numbers on the testing set from the realistic dataset? Do your classification results change at all with localization included?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "_Test the localization function on the images you captured in **Step 3**. Does the model accurately calculate a bounding box for the numbers in the images you found? If you did not use a graphical interface, you may need to investigate the bounding boxes by hand._ Provide an example of the localization created on a captured image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Optional Step 5: Build an Application or Program for a Model\n",
    "Take your project one step further. If you're interested, look to build an Android application or even a more robust Python program that can interface with input images and display the classified numbers and even the bounding boxes. You can for example try to build an augmented reality app by overlaying your answer on the image like the [Word Lens](https://en.wikipedia.org/wiki/Word_Lens) app does.\n",
    "\n",
    "Loading a TensorFlow model into a camera app on Android is demonstrated in the [TensorFlow Android demo app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android), which you can simply modify.\n",
    "\n",
    "If you decide to explore this optional route, be sure to document your interface and implementation, along with significant results you find. You can see the additional rubric items that you could be evaluated on by [following this link](https://review.udacity.com/#!/rubrics/413/view)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your optional code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "Provide additional documentation sufficient for detailing the implementation of the Android application or Python program for visualizing the classification of numbers in images. It should be clear how the program or application works. Demonstrations should be provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your documentation here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
